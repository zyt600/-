{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "KVAMnQR8xflq",
    "LLvOvdMbO9Qn",
    "7teo5qvRPnYP",
    "VPEcJ_L8QqZu",
    "fXPeeetKXvWh",
    "hOkdvhoeRHu3",
    "zbw-0N7KQ6B2",
    "MOfFQoClxPTl",
    "mDPbq8VmEvdB",
    "1HE235LrErqv"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 2\n",
    "You will tackle with a sentiment classification task using LSTM model and attention mechanism in this assigment."
   ],
   "metadata": {
    "id": "00iMtrQaVkW-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "Please make sure that you are using **GPU** to accelarate computation.\n",
    "\n",
    "Colab FAQ: https://research.google.com/colaboratory/faq.html"
   ],
   "metadata": {
    "id": "GS0Y5IA2T58y",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "id": "KVAMnQR8xflq",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import collections\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import T_co\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print('Using {} device'.format(device))\n",
    "# The assertion is to make sure GPU is available\n",
    "assert cuda == True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "# Yes, these are all necessary when you run experiments!\n",
    "seed = 1008\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "The script below will download the required sentiment analysis data.\n",
    "\n",
    "Data folder will be visible in the Colab file-explorer pane, which is loacted at left side of the page.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i\" -O data.zip\n",
    "# !unzip data.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Corpus\n",
    "Glove will be used as the word embedding tool in this assigment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess\n",
    "Preprocess data, then construct dataloader and vocabulary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Glove pretrained word embedding."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "# TODO\n",
    "vocab,embeddings = [],[]\n",
    "with open('glove.6B.50d.txt','rt',encoding=\"utf-8\") as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 50)\n"
     ]
    }
   ],
   "source": [
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400002, 50])\n"
     ]
    }
   ],
   "source": [
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construct your own vocabulary without other corpus.\n",
    "Hint: You should construct a vocabulary to map the word to index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<pad>', 0)\n",
      "('<unk>', 1)\n",
      "('the', 2)\n",
      "(',', 3)\n",
      "('.', 4)\n",
      "('of', 5)\n",
      "('to', 6)\n",
      "('and', 7)\n",
      "('in', 8)\n",
      "('a', 9)\n",
      "('\"', 10)\n",
      "(\"'s\", 11)\n",
      "('for', 12)\n",
      "('-', 13)\n",
      "('that', 14)\n",
      "('on', 15)\n",
      "('is', 16)\n",
      "('was', 17)\n",
      "('said', 18)\n",
      "('with', 19)\n",
      "('he', 20)\n",
      "('as', 21)\n",
      "('it', 22)\n",
      "('by', 23)\n",
      "('at', 24)\n",
      "('(', 25)\n",
      "(')', 26)\n",
      "('from', 27)\n",
      "('his', 28)\n",
      "(\"''\", 29)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "wordmap={}\n",
    "\n",
    "idx=0\n",
    "for item in vocab_npa:\n",
    "    wordmap[item]=idx\n",
    "    idx+=1\n",
    "\n",
    "showw=0\n",
    "for it in wordmap.items():\n",
    "    print(it)\n",
    "    showw+=1\n",
    "    if showw>=30:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "Load data and construct dataloader."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "data_dir = 'sentiment'\n",
    "trainTextPath=data_dir+os.sep+\"train_text.txt\"\n",
    "trainLabelPath=data_dir+os.sep+\"train_labels.txt\"\n",
    "\n",
    "testTextPath=data_dir+os.sep+\"test_text.txt\"\n",
    "testLabelPath=data_dir+os.sep+\"test_labels.txt\"\n",
    "\n",
    "valTextPath=data_dir+os.sep+\"val_text.txt\"\n",
    "valLabelPath=data_dir+os.sep+\"val_labels.txt\"\n",
    "\n",
    "maxSentenceLength=0\n",
    "with open(trainTextPath, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        if maxSentenceLength < len(sentence):\n",
    "            maxSentenceLength = len(sentence)\n",
    "\n",
    "with open(testTextPath, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        if maxSentenceLength < len(sentence):\n",
    "            maxSentenceLength = len(sentence)\n",
    "\n",
    "with open(valTextPath, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        if maxSentenceLength < len(sentence):\n",
    "            maxSentenceLength = len(sentence)\n",
    "\n",
    "\n",
    "maxSentenceLength+=1\n",
    "print(maxSentenceLength)\n",
    "\n",
    "padList=torch.zeros(maxSentenceLength,dtype=int)\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,textpath,labelpath):\n",
    "        self.vecList=[]\n",
    "        with open(textpath,mode=\"r\",encoding=\"utf-8\") as f:\n",
    "            tempList=f.read().lower().strip().split(\"\\n\")\n",
    "            # self.textList=tempList[:]\n",
    "            for line in tempList:\n",
    "                addList=padList.clone().detach()\n",
    "                line=line.split()\n",
    "                # print(\"length\",len(line))\n",
    "                for indx, itm in enumerate(line):\n",
    "                    if itm in wordmap:\n",
    "                        addList[indx]=wordmap[itm]\n",
    "                    else:\n",
    "                        addList[indx]=wordmap[\"<unk>\"]\n",
    "                self.vecList.append(addList.clone().detach())\n",
    "\n",
    "\n",
    "        with open(labelpath,mode=\"r\",encoding=\"utf-8\") as f:\n",
    "            self.labelList=f.read().strip().split(\"\\n\")\n",
    "            self.labelList=torch.tensor([int(i) for i in self.labelList])\n",
    "\n",
    "        if len(self.vecList)==len(self.labelList):\n",
    "            print(\"myDataset from\",textpath,\"is created,length is\",len(self.vecList))\n",
    "        else:\n",
    "            print(\"length different error\")\n",
    "    def __getitem__(self, index):\n",
    "        # print(\"----------\\n\",index,self.vecList[index],\"\\n----------\")\n",
    "        return self.vecList[index],self.labelList[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vecList)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myDataset from sentiment\\train_text.txt is created,length is 45615\n",
      "myDataset from sentiment\\val_text.txt is created,length is 2000\n",
      "myDataset from sentiment\\test_text.txt is created,length is 12284\n"
     ]
    }
   ],
   "source": [
    "trainDataset=myDataset(trainTextPath,trainLabelPath)\n",
    "valDataset=myDataset(valTextPath,valLabelPath)\n",
    "testDataset=myDataset(testTextPath,testLabelPath)\n",
    "\n",
    "trainDataLoader=DataLoader(trainDataset,batch_size=128,shuffle=True)\n",
    "valDataLoader=DataLoader(valDataset,batch_size=128,shuffle=False)\n",
    "testDataLoader=DataLoader(testDataset,batch_size=128,shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Zoo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
    "        super(BiRNN, self).__init__()\n",
    "        if pretrained_embedding is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size,device=device)\n",
    "        else:\n",
    "            self.embedding= nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding.clone().detach(), dtype=torch.float).clone().detach(), freeze=True).to(device)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True).to(device)\n",
    "        self.decoder = nn.Sequential(nn.Linear(4 * num_hiddens, num_hiddens).to(device),\n",
    "                                     nn.Linear(num_hiddens, 3)).to(device)\n",
    "        self.to(device)\n",
    "        print(\"init\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # print(\"forward\")\n",
    "\n",
    "        inputs=inputs.to(device)\n",
    "        self.to(device)\n",
    "        self.embedding.to(device)\n",
    "\n",
    "        embeddingOut = self.embedding(inputs)\n",
    "\n",
    "        self.encoder.flatten_parameters()\n",
    "        outputs, _ = self.encoder(embeddingOut)\n",
    "        encoding = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        softmax_fun=nn.Softmax(dim=1)\n",
    "        outs=softmax_fun(outs)\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [],
   "source": [
    "class BiRNN_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
    "        super(BiRNN_attention, self).__init__()\n",
    "        if pretrained_embedding is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float),\n",
    "                                                          freeze=True)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(2 * num_hiddens, 2 * num_hiddens))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(2 * num_hiddens, 1))\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear(2 * num_hiddens, num_hiddens),\n",
    "                                     nn.Linear(num_hiddens, 3))\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mask = 1 - torch.clamp(inputs, min=0, max=1)\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([0, 1, 2]))\n",
    "        u = torch.tanh(torch.matmul(states, self.weight_W))\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        att = att + mask.unsqueeze(2) * -1e7\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = states * att_score\n",
    "        encoding = torch.sum(scored_x, dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "You should train two models above with Glove pretrained word embedding and random initialized word embedding.\n",
    "\n",
    "Evaluation on the validation set and print out accuracy after training one epoch is required.\n",
    "\n",
    "You can tune some parameters and try different techniques, such as learning rate scheduler."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "outputs": [],
   "source": [
    "def train(model,train_loader=trainDataLoader,val_loader=valDataLoader,epoch=30,log_interval = 100):\n",
    "    print(\"statr training\")\n",
    "    for ep in range(epoch):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1,momentum=0.5)\n",
    "        lossFunction=nn.NLLLoss(reduction=\"sum\")\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            model.to(device)\n",
    "            data=data.to(device)\n",
    "            target=target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output=model.forward(data)\n",
    "            loss=lossFunction(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # TODO:还没写val\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                num_correct = 0\n",
    "                with torch.no_grad():\n",
    "                    for data, target in val_loader:\n",
    "                        data=data.to(device)\n",
    "                        target=target.to(device)\n",
    "                        output=model.forward(data)\n",
    "                        test_loss+=lossFunction(output,target)\n",
    "                        pred = output.data.max(1, keepdim=True)[1]\n",
    "                        # print(\"pred\",pred)\n",
    "                        # print(\"target\",target)\n",
    "                        num_correct+= pred.eq(target.data.view_as(pred)).sum()\n",
    "                avg_test_loss = test_loss/ len(val_loader.dataset)\n",
    "                print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                    avg_test_loss, num_correct, len(val_loader.dataset),\n",
    "                    100. * num_correct / len(val_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "outputs": [],
   "source": [
    "def test(model, test_loader=testDataLoader,device=device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data.to(device)\n",
    "            target.to(device)\n",
    "            output=model.forward(data)\n",
    "            lossFunction=nn.NLLLoss(reduction=\"sum\")\n",
    "            test_loss+=lossFunction(output,target)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            num_correct+= pred.eq(target.data.view_as(pred)).sum()\n",
    "    avg_test_loss = test_loss/ len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyt\\AppData\\Local\\Temp\\ipykernel_29644\\2684247167.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding= nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding.clone().detach(), dtype=torch.float).clone().detach(), freeze=True).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "statr training\n",
      "Train Epoch: 0 [0/45615 (0%)]\tLoss: -41.617737\n",
      "\n",
      "Val set: Average loss: -0.4125, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 0 [12800/45615 (28%)]\tLoss: -58.998375\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 0 [25600/45615 (56%)]\tLoss: -64.998550\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 0 [38400/45615 (84%)]\tLoss: -60.999130\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 1 [0/45615 (0%)]\tLoss: -49.000183\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 1 [12800/45615 (28%)]\tLoss: -57.999569\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 1 [25600/45615 (56%)]\tLoss: -62.999477\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 1 [38400/45615 (84%)]\tLoss: -50.000046\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 2 [0/45615 (0%)]\tLoss: -64.999588\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 2 [12800/45615 (28%)]\tLoss: -55.999912\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 2 [25600/45615 (56%)]\tLoss: -59.999878\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 2 [38400/45615 (84%)]\tLoss: -60.999855\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 3 [0/45615 (0%)]\tLoss: -62.999695\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 3 [12800/45615 (28%)]\tLoss: -50.000076\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 3 [25600/45615 (56%)]\tLoss: -55.000053\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 3 [38400/45615 (84%)]\tLoss: -68.999710\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 4 [0/45615 (0%)]\tLoss: -53.999916\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 4 [12800/45615 (28%)]\tLoss: -56.999950\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 4 [25600/45615 (56%)]\tLoss: -64.999825\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 4 [38400/45615 (84%)]\tLoss: -55.999912\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 5 [0/45615 (0%)]\tLoss: -59.999886\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 5 [12800/45615 (28%)]\tLoss: -66.999855\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 5 [25600/45615 (56%)]\tLoss: -52.000027\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 5 [38400/45615 (84%)]\tLoss: -46.000111\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 6 [0/45615 (0%)]\tLoss: -60.999905\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 6 [12800/45615 (28%)]\tLoss: -55.999958\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 6 [25600/45615 (56%)]\tLoss: -58.999950\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 6 [38400/45615 (84%)]\tLoss: -44.000088\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 7 [0/45615 (0%)]\tLoss: -59.999939\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 7 [12800/45615 (28%)]\tLoss: -59.999935\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 7 [25600/45615 (56%)]\tLoss: -57.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 7 [38400/45615 (84%)]\tLoss: -60.999943\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 8 [0/45615 (0%)]\tLoss: -52.999996\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 8 [12800/45615 (28%)]\tLoss: -58.999962\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 8 [25600/45615 (56%)]\tLoss: -58.999962\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 8 [38400/45615 (84%)]\tLoss: -54.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 9 [0/45615 (0%)]\tLoss: -54.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 9 [12800/45615 (28%)]\tLoss: -49.000042\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 9 [25600/45615 (56%)]\tLoss: -55.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 9 [38400/45615 (84%)]\tLoss: -54.999973\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 10 [0/45615 (0%)]\tLoss: -51.000008\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 10 [12800/45615 (28%)]\tLoss: -66.999931\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 10 [25600/45615 (56%)]\tLoss: -54.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 10 [38400/45615 (84%)]\tLoss: -51.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 11 [0/45615 (0%)]\tLoss: -62.999947\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 11 [12800/45615 (28%)]\tLoss: -52.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 11 [25600/45615 (56%)]\tLoss: -52.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 11 [38400/45615 (84%)]\tLoss: -59.999958\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 12 [0/45615 (0%)]\tLoss: -55.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 12 [12800/45615 (28%)]\tLoss: -56.999973\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 12 [25600/45615 (56%)]\tLoss: -51.000008\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 12 [38400/45615 (84%)]\tLoss: -47.000019\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 13 [0/45615 (0%)]\tLoss: -55.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 13 [12800/45615 (28%)]\tLoss: -54.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 13 [25600/45615 (56%)]\tLoss: -54.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 13 [38400/45615 (84%)]\tLoss: -55.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 14 [0/45615 (0%)]\tLoss: -53.000008\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 14 [12800/45615 (28%)]\tLoss: -51.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 14 [25600/45615 (56%)]\tLoss: -69.999939\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 14 [38400/45615 (84%)]\tLoss: -60.999969\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 15 [0/45615 (0%)]\tLoss: -58.999977\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 15 [12800/45615 (28%)]\tLoss: -59.999962\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 15 [25600/45615 (56%)]\tLoss: -48.000015\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 15 [38400/45615 (84%)]\tLoss: -52.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 16 [0/45615 (0%)]\tLoss: -56.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 16 [12800/45615 (28%)]\tLoss: -51.000000\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 16 [25600/45615 (56%)]\tLoss: -53.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 16 [38400/45615 (84%)]\tLoss: -59.999969\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 17 [0/45615 (0%)]\tLoss: -50.000000\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 17 [12800/45615 (28%)]\tLoss: -53.000008\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 17 [25600/45615 (56%)]\tLoss: -58.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 17 [38400/45615 (84%)]\tLoss: -58.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 18 [0/45615 (0%)]\tLoss: -55.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 18 [12800/45615 (28%)]\tLoss: -56.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 18 [25600/45615 (56%)]\tLoss: -54.000000\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 18 [38400/45615 (84%)]\tLoss: -63.999973\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 19 [0/45615 (0%)]\tLoss: -54.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 19 [12800/45615 (28%)]\tLoss: -55.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 19 [25600/45615 (56%)]\tLoss: -66.999962\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 19 [38400/45615 (84%)]\tLoss: -60.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 20 [0/45615 (0%)]\tLoss: -51.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 20 [12800/45615 (28%)]\tLoss: -54.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 20 [25600/45615 (56%)]\tLoss: -59.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 20 [38400/45615 (84%)]\tLoss: -65.999969\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 21 [0/45615 (0%)]\tLoss: -69.999969\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 21 [12800/45615 (28%)]\tLoss: -55.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 21 [25600/45615 (56%)]\tLoss: -57.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 21 [38400/45615 (84%)]\tLoss: -71.999954\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 22 [0/45615 (0%)]\tLoss: -62.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 22 [12800/45615 (28%)]\tLoss: -52.000004\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 22 [25600/45615 (56%)]\tLoss: -56.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 22 [38400/45615 (84%)]\tLoss: -64.999969\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 23 [0/45615 (0%)]\tLoss: -61.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 23 [12800/45615 (28%)]\tLoss: -56.999996\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 23 [25600/45615 (56%)]\tLoss: -65.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 23 [38400/45615 (84%)]\tLoss: -62.999973\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 24 [0/45615 (0%)]\tLoss: -54.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 24 [12800/45615 (28%)]\tLoss: -46.000015\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 24 [25600/45615 (56%)]\tLoss: -50.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 24 [38400/45615 (84%)]\tLoss: -59.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 25 [0/45615 (0%)]\tLoss: -64.999977\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 25 [12800/45615 (28%)]\tLoss: -60.999977\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 25 [25600/45615 (56%)]\tLoss: -55.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 25 [38400/45615 (84%)]\tLoss: -60.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 26 [0/45615 (0%)]\tLoss: -59.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 26 [12800/45615 (28%)]\tLoss: -58.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 26 [25600/45615 (56%)]\tLoss: -67.999977\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 26 [38400/45615 (84%)]\tLoss: -55.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 27 [0/45615 (0%)]\tLoss: -49.000004\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 27 [12800/45615 (28%)]\tLoss: -57.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 27 [25600/45615 (56%)]\tLoss: -58.999989\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 27 [38400/45615 (84%)]\tLoss: -51.999996\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 28 [0/45615 (0%)]\tLoss: -49.000008\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 28 [12800/45615 (28%)]\tLoss: -53.999996\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 28 [25600/45615 (56%)]\tLoss: -59.999985\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 28 [38400/45615 (84%)]\tLoss: -54.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 29 [0/45615 (0%)]\tLoss: -52.000000\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 29 [12800/45615 (28%)]\tLoss: -57.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 29 [25600/45615 (56%)]\tLoss: -62.999981\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n",
      "Train Epoch: 29 [38400/45615 (84%)]\tLoss: -58.999992\n",
      "\n",
      "Val set: Average loss: -0.4345, Accuracy: 869/2000 (43%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train BiRNN with Glove pretrained word embedding\n",
    "# TODO\n",
    "# num_hiddens随便大小吗\n",
    "myBiRNNpretrained=BiRNN(vocab_size=len(wordmap),embed_size=50,num_hiddens=5,num_layers=2,pretrained_embedding=my_embedding_layer.weight.clone().detach())\n",
    "train(myBiRNNpretrained)\n",
    "# test(myBiRNNpretrained)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (514093880.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [412]\u001B[1;36m\u001B[0m\n\u001B[1;33m    with myBiRNNpretrained.\u001B[0m\n\u001B[1;37m                          ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "with myBiRNNpretrained."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with myBiRNNpretrained."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN without pretrained word embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "6rXFCb56GBOK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN_attention with Glove pretrained embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "aeetQ2uZGBXf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN_attention without pretrained word embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "uiRSuAy0GBdJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Report (optional)\n",
    "You can briefly report what strategies you attempted in this assignment."
   ],
   "metadata": {
    "id": "Cpjn8StjHcY9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}