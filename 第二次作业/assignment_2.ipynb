{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "KVAMnQR8xflq",
    "LLvOvdMbO9Qn",
    "7teo5qvRPnYP",
    "VPEcJ_L8QqZu",
    "fXPeeetKXvWh",
    "hOkdvhoeRHu3",
    "zbw-0N7KQ6B2",
    "MOfFQoClxPTl",
    "mDPbq8VmEvdB",
    "1HE235LrErqv"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 2\n",
    "You will tackle with a sentiment classification task using LSTM model and attention mechanism in this assigment."
   ],
   "metadata": {
    "id": "00iMtrQaVkW-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "Please make sure that you are using **GPU** to accelarate computation.\n",
    "\n",
    "Colab FAQ: https://research.google.com/colaboratory/faq.html"
   ],
   "metadata": {
    "id": "GS0Y5IA2T58y",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "id": "KVAMnQR8xflq",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "import collections\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import T_co\n",
    "# from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "6Kk2dEh8--MH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print('Using {} device'.format(device))\n",
    "# The assertion is to make sure GPU is available\n",
    "assert cuda == True"
   ],
   "metadata": {
    "id": "MdjDF7WRF7UU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUsing \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m device\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(device))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# The assertion is to make sure GPU is available\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m cuda \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "# Yes, these are all necessary when you run experiments!\n",
    "seed = 1008\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ],
   "metadata": {
    "id": "wXkKqcxiO6b3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "The script below will download the required sentiment analysis data.\n",
    "\n",
    "Data folder will be visible in the Colab file-explorer pane, which is loacted at left side of the page.\n"
   ],
   "metadata": {
    "id": "LLvOvdMbO9Qn",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i\" -O data.zip\n",
    "# !unzip data.zip"
   ],
   "metadata": {
    "id": "SxNT5mN-rHGD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Corpus\n",
    "Glove will be used as the word embedding tool in this assigment."
   ],
   "metadata": {
    "id": "7teo5qvRPnYP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ],
   "metadata": {
    "id": "34dba6GT_EhP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess\n",
    "Preprocess data, then construct dataloader and vocabulary."
   ],
   "metadata": {
    "id": "VPEcJ_L8QqZu",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Glove pretrained word embedding."
   ],
   "metadata": {
    "id": "fXPeeetKXvWh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "vocab,embeddings = [],[]\n",
    "with open('glove.6B.300d.txt','rt',encoding=\"utf-8\") as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)"
   ],
   "metadata": {
    "id": "nADNNIT11U-P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 300)\n"
     ]
    }
   ],
   "source": [
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "wordmap={}\n",
    "\n",
    "idx=0\n",
    "for item in vocab_npa:\n",
    "    wordmap[item]=idx\n",
    "    idx+=1\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400002, 300])\n"
     ]
    }
   ],
   "source": [
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construct your own vocabulary without other corpus.\n",
    "Hint: You should construct a vocabulary to map the word to index."
   ],
   "metadata": {
    "id": "hOkdvhoeRHu3",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO"
   ],
   "metadata": {
    "id": "FXgxiqptBsAK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "Load data and construct dataloader."
   ],
   "metadata": {
    "id": "zbw-0N7KQ6B2",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO"
   ],
   "metadata": {
    "id": "p5zh-qrS_aAl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myDataset from sentiment\\train_text.txt is created,length is 45615\n",
      "myDataset from sentiment\\val_text.txt is created,length is 2000\n",
      "myDataset from sentiment\\test_text.txt is created,length is 12284\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "data_dir = 'sentiment'\n",
    "trainTextPath=data_dir+os.sep+\"train_text.txt\"\n",
    "trainLabelPath=data_dir+os.sep+\"train_labels.txt\"\n",
    "\n",
    "testTextPath=data_dir+os.sep+\"test_text.txt\"\n",
    "testLabelPath=data_dir+os.sep+\"test_labels.txt\"\n",
    "\n",
    "valTextPath=data_dir+os.sep+\"val_text.txt\"\n",
    "valLabelPath=data_dir+os.sep+\"val_labels.txt\"\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,textpath,labelpath):\n",
    "        f=open(textpath,mode=\"r\",encoding=\"utf-8\")\n",
    "        self.textList=f.read().strip().split(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "        f=open(labelpath,mode=\"r\")\n",
    "        self.labelList=f.read().strip().split(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "        if len(self.textList)==len(self.labelList):\n",
    "            print(\"myDataset from\",textpath,\"is created,length is\",len(self.textList))\n",
    "        else:\n",
    "            print(\"length different error\")\n",
    "    def __getitem__(self, index):\n",
    "        return self.textList[index],self.labelList[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.textList)\n",
    "\n",
    "trainDataset=myDataset(trainTextPath,trainLabelPath)\n",
    "valDataset=myDataset(valTextPath,valLabelPath)\n",
    "testDataset=myDataset(testTextPath,testLabelPath)\n",
    "\n",
    "trainDataLoader=DataLoader(trainDataset,batch_size=128,shuffle=True)\n",
    "valDataLoader=DataLoader(valDataset,batch_size=128,shuffle=True)\n",
    "testDataLoader=DataLoader(testDataset,batch_size=128,shuffle=False)\n",
    "\n",
    "# 输出一个batch的数据看一看\n",
    "# train_features, train_labels = next(iter(trainDataLoader))\n",
    "# print(\"\\ntrain_features:\\n\\n\",train_features,\"\\n\\ntrain_labels:\\n\" ,train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Zoo"
   ],
   "metadata": {
    "id": "mDPbq8VmEvdB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
    "        super(BiRNN, self).__init__()\n",
    "        if pretrained_embedding is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        else:\n",
    "            self.embedding= nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float), freeze=True)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.decoder = nn.Sequential(nn.Linear(4 * num_hiddens, num_hiddens),\n",
    "                                     nn.Linear(num_hiddens, 3))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        self.encoder.flatten_parameters()\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        encoding = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ],
   "metadata": {
    "id": "vTyWAUr81oR9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BiRNN_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
    "        super(BiRNN_attention, self).__init__()\n",
    "        if pretrained_embedding is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float),\n",
    "                                                          freeze=True)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(2 * num_hiddens, 2 * num_hiddens))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(2 * num_hiddens, 1))\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear(2 * num_hiddens, num_hiddens),\n",
    "                                     nn.Linear(num_hiddens, 3))\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mask = 1 - torch.clamp(inputs, min=0, max=1)\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([0, 1, 2]))\n",
    "        u = torch.tanh(torch.matmul(states, self.weight_W))\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        att = att + mask.unsqueeze(2) * -1e7\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = states * att_score\n",
    "        encoding = torch.sum(scored_x, dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "id": "WBXSkgb41oVF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "You should train two models above with Glove pretrained word embedding and random initialized word embedding.\n",
    "\n",
    "Evaluation on the validation set and print out accuracy after training one epoch is required.\n",
    "\n",
    "You can tune some parameters and try different techniques, such as learning rate scheduler."
   ],
   "metadata": {
    "id": "1HE235LrErqv",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(model,train_loader,epoch,log_interval = 100):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.5)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data.to(device)\n",
    "        target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model.forward(data)\n",
    "        lossFunction=nn.NLLLoss()\n",
    "        loss=lossFunction(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # TODO:还没写val\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data.to(device)\n",
    "            target.to(device)\n",
    "            output=model.forward(data)\n",
    "            lossFunction=nn.NLLLoss(reduction=\"sum\")\n",
    "            test_loss+=lossFunction(output,target)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            num_correct+= pred.eq(target.data.view_as(pred)).sum()\n",
    "    avg_test_loss = test_loss/ len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train BiRNN with Glove pretrained word embedding\n",
    "# TODO\n",
    "myBiRNNpretrained=BiRNN(vocab_size=400000,embed_size=300,)\n",
    "train(myBiRNNpretrained,trainDataLoader,2000)\n",
    "test(myBiRNNpretrained,testDataLoader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with myBiRNNpretrained."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN without pretrained word embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "6rXFCb56GBOK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN_attention with Glove pretrained embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "aeetQ2uZGBXf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN_attention without pretrained word embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "uiRSuAy0GBdJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Report (optional)\n",
    "You can briefly report what strategies you attempted in this assignment."
   ],
   "metadata": {
    "id": "Cpjn8StjHcY9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}