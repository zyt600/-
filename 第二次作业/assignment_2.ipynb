{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KVAMnQR8xflq",
        "LLvOvdMbO9Qn",
        "7teo5qvRPnYP",
        "VPEcJ_L8QqZu",
        "fXPeeetKXvWh",
        "hOkdvhoeRHu3",
        "zbw-0N7KQ6B2",
        "MOfFQoClxPTl",
        "mDPbq8VmEvdB",
        "1HE235LrErqv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "You will tackle with a sentiment classification task using LSTM model and attention mechanism in this assigment."
      ],
      "metadata": {
        "id": "00iMtrQaVkW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "Please make sure that you are using **GPU** to accelarate computation.\n",
        "\n",
        "Colab FAQ: https://research.google.com/colaboratory/faq.html"
      ],
      "metadata": {
        "id": "GS0Y5IA2T58y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies"
      ],
      "metadata": {
        "id": "KVAMnQR8xflq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import collections\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Kk2dEh8--MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your device \n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "\n",
        "# The assertion is to make sure GPU is available\n",
        "assert cuda == True"
      ],
      "metadata": {
        "id": "MdjDF7WRF7UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up random seed to 1008. Do not change the random seed.\n",
        "# Yes, these are all necessary when you run experiments!\n",
        "seed = 1008\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n"
      ],
      "metadata": {
        "id": "wXkKqcxiO6b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "The script below will download the required sentiment analysis data.\n",
        "\n",
        "Data folder will be visible in the Colab file-explorer pane, which is loacted at left side of the page.\n"
      ],
      "metadata": {
        "id": "LLvOvdMbO9Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i' -O data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "SxNT5mN-rHGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus\n",
        "Glove will be used as the word embedding tool in this assigment."
      ],
      "metadata": {
        "id": "7teo5qvRPnYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "34dba6GT_EhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess\n",
        "Preprocess data, then construct dataloader and vocabulary."
      ],
      "metadata": {
        "id": "VPEcJ_L8QqZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Glove pretrained word embedding."
      ],
      "metadata": {
        "id": "fXPeeetKXvWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "nADNNIT11U-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct your own vocabulary without other corpus.\n",
        "Hint: You should construct a vocabulary to map the word to index."
      ],
      "metadata": {
        "id": "hOkdvhoeRHu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "FXgxiqptBsAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "Load data and construct dataloader."
      ],
      "metadata": {
        "id": "zbw-0N7KQ6B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'sentiment'\n",
        "# TODO"
      ],
      "metadata": {
        "id": "p5zh-qrS_aAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "Bidirectional LSTM and attention mechanism will be used in this section."
      ],
      "metadata": {
        "id": "MOfFQoClxPTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Zoo"
      ],
      "metadata": {
        "id": "mDPbq8VmEvdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
        "        super(BiRNN, self).__init__()\n",
        "        if pretrained_embedding is None:\n",
        "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        else:\n",
        "            self.embedding= nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float), freeze=True)\n",
        "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.decoder = nn.Sequential(nn.Linear(4 * num_hiddens, num_hiddens), nn.Linear(num_hiddens, 3))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        self.encoder.flatten_parameters()\n",
        "        outputs, _ = self.encoder(embeddings)\n",
        "        encoding = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)\n",
        "        outs = self.decoder(encoding)\n",
        "        return outs"
      ],
      "metadata": {
        "id": "vTyWAUr81oR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiRNN_attention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
        "        super(BiRNN_attention, self).__init__()\n",
        "        if pretrained_embedding is None:\n",
        "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float),\n",
        "                                                          freeze=True)\n",
        "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.weight_W = nn.Parameter(torch.Tensor(2 * num_hiddens, 2 * num_hiddens))\n",
        "        self.weight_proj = nn.Parameter(torch.Tensor(2 * num_hiddens, 1))\n",
        "\n",
        "        self.decoder = nn.Sequential(nn.Linear(2 * num_hiddens, num_hiddens), nn.Linear(num_hiddens, 3))\n",
        "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        mask = 1 - torch.clamp(inputs, min=0, max=1)\n",
        "        embeddings = self.embedding(inputs)\n",
        "        states, hidden = self.encoder(embeddings.permute([0, 1, 2]))\n",
        "        u = torch.tanh(torch.matmul(states, self.weight_W))\n",
        "        att = torch.matmul(u, self.weight_proj)\n",
        "        att = att + mask.unsqueeze(2) * -1e7\n",
        "        att_score = F.softmax(att, dim=1)\n",
        "        scored_x = states * att_score\n",
        "        encoding = torch.sum(scored_x, dim=1)\n",
        "        outputs = self.decoder(encoding)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "WBXSkgb41oVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "You should train two models above with Glove pretrained word embedding and random initialized word embedding.\n",
        "\n",
        "Evaluation on the validation set and print out accuracy after training one epoch is required.\n",
        "\n",
        "You can tune some parameters and try different techniques, such as learning rate scheduler."
      ],
      "metadata": {
        "id": "1HE235LrErqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN with Glove pretrained word embedding\n",
        "# TODO"
      ],
      "metadata": {
        "id": "j3gbyRUXEPZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN without pretrained word embedding\n",
        "# TODO"
      ],
      "metadata": {
        "id": "6rXFCb56GBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN_attention with Glove pretrained embedding\n",
        "# TODO"
      ],
      "metadata": {
        "id": "aeetQ2uZGBXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BiRNN_attention without pretrained word embedding\n",
        "# TODO"
      ],
      "metadata": {
        "id": "uiRSuAy0GBdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report (optional)\n",
        "You can briefly report what strategies you attempted in this assignment."
      ],
      "metadata": {
        "id": "Cpjn8StjHcY9"
      }
    }
  ]
}