{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "KVAMnQR8xflq",
    "LLvOvdMbO9Qn",
    "7teo5qvRPnYP",
    "VPEcJ_L8QqZu",
    "fXPeeetKXvWh",
    "hOkdvhoeRHu3",
    "zbw-0N7KQ6B2",
    "MOfFQoClxPTl",
    "mDPbq8VmEvdB",
    "1HE235LrErqv"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 2\n",
    "You will tackle with a sentiment classification task using LSTM model and attention mechanism in this assigment."
   ],
   "metadata": {
    "id": "00iMtrQaVkW-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "Please make sure that you are using **GPU** to accelarate computation.\n",
    "\n",
    "Colab FAQ: https://research.google.com/colaboratory/faq.html"
   ],
   "metadata": {
    "id": "GS0Y5IA2T58y",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {
    "id": "KVAMnQR8xflq",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import collections\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import T_co\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print('Using {} device'.format(device))\n",
    "# The assertion is to make sure GPU is available\n",
    "assert cuda == True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "# Yes, these are all necessary when you run experiments!\n",
    "seed = 1008\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "The script below will download the required sentiment analysis data.\n",
    "\n",
    "Data folder will be visible in the Colab file-explorer pane, which is loacted at left side of the page.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=1jqYJ9jhjukhXvEk4GnMAPYE-SvhSG24i\" -O data.zip\n",
    "# !unzip data.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Corpus\n",
    "Glove will be used as the word embedding tool in this assigment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess\n",
    "Preprocess data, then construct dataloader and vocabulary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Glove pretrained word embedding."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# TODO\n",
    "vocab,embeddings = [],[]\n",
    "with open('glove.6B.50d.txt','rt',encoding=\"utf-8\") as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>' '<unk>' 'the' ',' '.' 'of' 'to' 'and' 'in' 'a']\n",
      "(400002, 50)\n"
     ]
    }
   ],
   "source": [
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "#insert '<pad>' and '<unk>' tokens at start of vocab_npa.\n",
    "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
    "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
    "print(vocab_npa[:10])\n",
    "\n",
    "\n",
    "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
    "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
    "print(embs_npa.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400002, 50])\n"
     ]
    }
   ],
   "source": [
    "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "\n",
    "assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "print(my_embedding_layer.weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construct your own vocabulary without other corpus.\n",
    "Hint: You should construct a vocabulary to map the word to index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<pad>', 0)\n",
      "('<unk>', 1)\n",
      "('the', 2)\n",
      "(',', 3)\n",
      "('.', 4)\n",
      "('of', 5)\n",
      "('to', 6)\n",
      "('and', 7)\n",
      "('in', 8)\n",
      "('a', 9)\n",
      "('\"', 10)\n",
      "(\"'s\", 11)\n",
      "('for', 12)\n",
      "('-', 13)\n",
      "('that', 14)\n",
      "('on', 15)\n",
      "('is', 16)\n",
      "('was', 17)\n",
      "('said', 18)\n",
      "('with', 19)\n",
      "('he', 20)\n",
      "('as', 21)\n",
      "('it', 22)\n",
      "('by', 23)\n",
      "('at', 24)\n",
      "('(', 25)\n",
      "(')', 26)\n",
      "('from', 27)\n",
      "('his', 28)\n",
      "(\"''\", 29)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "wordmap={}\n",
    "\n",
    "idx=0\n",
    "for item in vocab_npa:\n",
    "    wordmap[item]=idx\n",
    "    idx+=1\n",
    "\n",
    "showw=0\n",
    "for it in wordmap.items():\n",
    "    print(it)\n",
    "    showw+=1\n",
    "    if showw>=30:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "Load data and construct dataloader."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "data_dir = 'sentiment'\n",
    "trainTextPath=data_dir+os.sep+\"train_text.txt\"\n",
    "trainLabelPath=data_dir+os.sep+\"train_labels.txt\"\n",
    "\n",
    "testTextPath=data_dir+os.sep+\"test_text.txt\"\n",
    "testLabelPath=data_dir+os.sep+\"test_labels.txt\"\n",
    "\n",
    "valTextPath=data_dir+os.sep+\"val_text.txt\"\n",
    "valLabelPath=data_dir+os.sep+\"val_labels.txt\"\n",
    "\n",
    "maxSentenceLength=0\n",
    "with open(trainTextPath, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        if maxSentenceLength < len(sentence):\n",
    "            maxSentenceLength = len(sentence)\n",
    "\n",
    "with open(testTextPath, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        if maxSentenceLength < len(sentence):\n",
    "            maxSentenceLength = len(sentence)\n",
    "\n",
    "with open(valTextPath, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().strip().split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        if maxSentenceLength < len(sentence):\n",
    "            maxSentenceLength = len(sentence)\n",
    "\n",
    "\n",
    "maxSentenceLength+=1\n",
    "print(maxSentenceLength)\n",
    "\n",
    "padList=torch.zeros(maxSentenceLength,dtype=int)\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,textpath,labelpath):\n",
    "        self.vecList=[]\n",
    "        with open(textpath,mode=\"r\",encoding=\"utf-8\") as f:\n",
    "            tempList=f.read().lower().strip().split(\"\\n\")\n",
    "            # self.textList=tempList[:]\n",
    "            for line in tempList:\n",
    "                addList=padList.clone().detach()\n",
    "                line=line.split()\n",
    "                # print(\"length\",len(line))\n",
    "                for indx, itm in enumerate(line):\n",
    "                    if itm in wordmap:\n",
    "                        addList[indx]=wordmap[itm]\n",
    "                    else:\n",
    "                        addList[indx]=wordmap[\"<unk>\"]\n",
    "                self.vecList.append(addList.clone().detach())\n",
    "\n",
    "\n",
    "        with open(labelpath,mode=\"r\",encoding=\"utf-8\") as f:\n",
    "            self.labelList=f.read().strip().split(\"\\n\")\n",
    "            self.labelList=torch.tensor([int(i) for i in self.labelList])\n",
    "\n",
    "        if len(self.vecList)==len(self.labelList):\n",
    "            print(\"myDataset from\",textpath,\"is created,length is\",len(self.vecList))\n",
    "        else:\n",
    "            print(\"length different error\")\n",
    "    def __getitem__(self, index):\n",
    "        # print(\"----------\\n\",index,self.vecList[index],\"\\n----------\")\n",
    "        return self.vecList[index],self.labelList[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vecList)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myDataset from sentiment\\train_text.txt is created,length is 45615\n",
      "myDataset from sentiment\\val_text.txt is created,length is 2000\n",
      "myDataset from sentiment\\test_text.txt is created,length is 12284\n"
     ]
    }
   ],
   "source": [
    "trainDataset=myDataset(trainTextPath,trainLabelPath)\n",
    "valDataset=myDataset(valTextPath,valLabelPath)\n",
    "testDataset=myDataset(testTextPath,testLabelPath)\n",
    "\n",
    "trainDataLoader=DataLoader(trainDataset,batch_size=128,shuffle=True)\n",
    "valDataLoader=DataLoader(valDataset,batch_size=128,shuffle=False)\n",
    "testDataLoader=DataLoader(testDataset,batch_size=128,shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Zoo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
    "        super(BiRNN, self).__init__()\n",
    "        if pretrained_embedding is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size,device=device)\n",
    "        else:\n",
    "            self.embedding= nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float), freeze=True)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.decoder = nn.Sequential(nn.Linear(4 * num_hiddens, num_hiddens),\n",
    "                                     nn.Linear(num_hiddens, 3))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs.to(device)\n",
    "        self.to(device)\n",
    "        print(next(self.parameters()).device)\n",
    "        embeddingOut = self.embedding(inputs)\n",
    "        self.encoder.flatten_parameters()\n",
    "        outputs, _ = self.encoder(embeddingOut)\n",
    "        encoding = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "class BiRNN_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
    "        super(BiRNN_attention, self).__init__()\n",
    "        if pretrained_embedding is None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding, dtype=torch.float),\n",
    "                                                          freeze=True)\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(2 * num_hiddens, 2 * num_hiddens))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(2 * num_hiddens, 1))\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear(2 * num_hiddens, num_hiddens),\n",
    "                                     nn.Linear(num_hiddens, 3))\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mask = 1 - torch.clamp(inputs, min=0, max=1)\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([0, 1, 2]))\n",
    "        u = torch.tanh(torch.matmul(states, self.weight_W))\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        att = att + mask.unsqueeze(2) * -1e7\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = states * att_score\n",
    "        encoding = torch.sum(scored_x, dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "You should train two models above with Glove pretrained word embedding and random initialized word embedding.\n",
    "\n",
    "Evaluation on the validation set and print out accuracy after training one epoch is required.\n",
    "\n",
    "You can tune some parameters and try different techniques, such as learning rate scheduler."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "def train(model,train_loader=trainDataLoader,val_loader=valDataLoader,epoch=5000,log_interval = 100,device=device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.5)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data.to(device)\n",
    "        target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output=model.forward(data)\n",
    "        lossFunction=nn.NLLLoss()\n",
    "        loss=lossFunction(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # TODO:还没写val\n",
    "        # if batch_idx % log_interval == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "        #     model.eval()\n",
    "        #     test_loss = 0\n",
    "        #     num_correct = 0\n",
    "        #     with torch.no_grad():\n",
    "        #         for data, target in val_loader:\n",
    "        #             data.to(device)\n",
    "        #             target.to(device)\n",
    "        #             output=model.forward(data)\n",
    "        #             lossFunction=nn.NLLLoss(reduction=\"sum\")\n",
    "        #             test_loss+=lossFunction(output,target)\n",
    "        #             pred = output.data.max(1, keepdim=True)[1]\n",
    "        #             num_correct+= pred.eq(target.data.view_as(pred)).sum()\n",
    "        #     avg_test_loss = test_loss/ len(val_loader.dataset)\n",
    "        #     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        #         avg_test_loss, num_correct, len(val_loader.dataset),\n",
    "        #         100. * num_correct / len(val_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def test(model, test_loader=testDataLoader,device=device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data.to(device)\n",
    "            target.to(device)\n",
    "            output=model.forward(data)\n",
    "            lossFunction=nn.NLLLoss(reduction=\"sum\")\n",
    "            test_loss+=lossFunction(output,target)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            num_correct+= pred.eq(target.data.view_as(pred)).sum()\n",
    "    avg_test_loss = test_loss/ len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [195]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train BiRNN with Glove pretrained word embedding\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# TODO\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# num_hiddens随便大小吗\u001B[39;00m\n\u001B[0;32m      4\u001B[0m myBiRNNpretrained\u001B[38;5;241m=\u001B[39mBiRNN(vocab_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(wordmap),embed_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,num_hiddens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,num_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmyBiRNNpretrained\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [193]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, val_loader, epoch, log_interval, device)\u001B[0m\n\u001B[0;32m      8\u001B[0m target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 11\u001B[0m output\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m lossFunction\u001B[38;5;241m=\u001B[39mnn\u001B[38;5;241m.\u001B[39mNLLLoss()\n\u001B[0;32m     13\u001B[0m loss\u001B[38;5;241m=\u001B[39mlossFunction(output,target)\n",
      "Input \u001B[1;32mIn [191]\u001B[0m, in \u001B[0;36mBiRNN.forward\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters())\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 18\u001B[0m embeddingOut \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mflatten_parameters()\n\u001B[0;32m     20\u001B[0m outputs, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(embeddingOut)\n",
      "File \u001B[1;32mD:\\anacccnew\\envs\\mul2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\anacccnew\\envs\\mul2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anacccnew\\envs\\mul2\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2193\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2194\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2195\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2196\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2197\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2198\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "# Train BiRNN with Glove pretrained word embedding\n",
    "# TODO\n",
    "# num_hiddens随便大小吗\n",
    "myBiRNNpretrained=BiRNN(vocab_size=len(wordmap),embed_size=50,num_hiddens=5,num_layers=2)\n",
    "train(myBiRNNpretrained)\n",
    "# test(myBiRNNpretrained)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with myBiRNNpretrained."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with myBiRNNpretrained."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN without pretrained word embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "6rXFCb56GBOK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN_attention with Glove pretrained embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "aeetQ2uZGBXf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train BiRNN_attention without pretrained word embedding\n",
    "# TODO"
   ],
   "metadata": {
    "id": "uiRSuAy0GBdJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Report (optional)\n",
    "You can briefly report what strategies you attempted in this assignment."
   ],
   "metadata": {
    "id": "Cpjn8StjHcY9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}